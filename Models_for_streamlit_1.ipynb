{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KmwRxkU9cC7P"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "text = requests.get(url).text\n",
        "\n",
        "print(\"Length:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8AUroUucOus",
        "outputId": "279db878-684a-468a-d84e-03843b03e6af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 593731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering out just the story part\n",
        "start_idx, end_idx = None, None\n",
        "\n",
        "#finding where the first story begins and ends indexes\n",
        "lines = text.splitlines()\n",
        "for i, line in enumerate(lines):\n",
        "    if \" A SCANDAL IN BOHEMIA\" in line:\n",
        "        start_idx = i\n",
        "        break\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    if \"END OF THE PROJECT GUTENBERG\" in line.upper():\n",
        "        end_idx = i\n",
        "        break\n",
        "\n",
        "# getting the text that is between the markers\n",
        "if start_idx is not None and end_idx is not None:\n",
        "    story_lines = lines[start_idx:end_idx]\n",
        "elif start_idx is not None:\n",
        "    story_lines = lines[start_idx:]\n",
        "else:\n",
        "    story_lines = lines\n",
        "\n",
        "text = \"\\n\".join(story_lines).strip()\n",
        "\n",
        "print(f\"Length: {len(text)}\\n\")\n",
        "print(text[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxkyyJ8VcPuJ",
        "outputId": "09eb0943-fd18-4a86-9ec6-61ff293e8aaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 561653\n",
            "\n",
            "I. A SCANDAL IN BOHEMIA\n",
            "\n",
            "\n",
            "I.\n",
            "\n",
            "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
            "mention her under any other name. In his eyes she eclipses and\n",
            "predominates the whole of her sex. It was not that he felt any emotion\n",
            "akin to love for Irene Adler. All emotions, and that one particularly,\n",
            "were abhorrent to his cold, precise but admirably balanced mind. He\n",
            "was, I take it, the most perfect reasoning and observing machine that\n",
            "the world has seen, but as a lover he would have placed himself in a\n",
            "false position. He never spoke of the softer passions, save with a gibe\n",
            "and a sneer. They were admirable things for the observer—excellent for\n",
            "drawing the veil from men’s motives and actions. But for the trained\n",
            "reasoner to admit such intrusions into his own delicate and finely\n",
            "adjusted \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering out everything other than letters numbers and .\n",
        "\n",
        "cleaned_text = re.sub(r'[^a-zA-Z0-9 \\.]', '', text)\n",
        "cleaned_text = cleaned_text.lower() #converting everything to lower case\n",
        "len(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2RLDDvtcVGK",
        "outputId": "b355a9bc-4e75-466a-cdad-be3b01b340c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "533032"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into tokens(words)\n",
        "cleaned = re.sub(r'[^a-zA-Z0-9\\.]', ' ', text)\n",
        "cleaned = re.sub(r'(\\.)', r' \\1 ', cleaned)  #space around every period\n",
        "cleaned = re.sub(r'\\s+', ' ', cleaned).strip().lower()\n",
        "tokens = cleaned.split()#seperated .\n",
        "print(len(tokens))\n",
        "print(tokens[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sfzoru2cWba",
        "outputId": "1857eb6e-c8c8-41de-a99d-2e8ac0e2e3d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112095\n",
            "['i', '.', 'a', 'scandal', 'in', 'bohemia', 'i', '.', 'to', 'sherlock', 'holmes', 'she', 'is', 'always', 'the', 'woman', '.', 'i', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'in', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'it', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep '.' in tokens\n",
        "tokens_wo_period = tokens  # don't remove '.'\n",
        "\n",
        "# Build vocab (include <unk>)\n",
        "vocab = sorted(set(tokens_wo_period) | {\"<unk>\"})\n",
        "\n",
        "# Frequency stats\n",
        "from collections import Counter\n",
        "freq = Counter(tokens_wo_period)\n",
        "most_common_10 = freq.most_common(10)\n",
        "least_common_10 = freq.most_common()[-10:]\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\\n\")\n",
        "print(\"10 most frequent words:\")\n",
        "for w, c in most_common_10:\n",
        "    print(f\"{w:>10} : {c}\")\n",
        "\n",
        "print(\"\\n10 least frequent words:\")\n",
        "for w, c in least_common_10:\n",
        "    print(f\"{w:>10} : {c}\")\n",
        "\n",
        "# Create mappings\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "# Ensure <unk> present (safety)\n",
        "if \"<unk>\" not in word_to_idx:\n",
        "    unk_index = len(word_to_idx)\n",
        "    word_to_idx[\"<unk>\"] = unk_index\n",
        "    idx_to_word[unk_index] = \"<unk>\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzcTVykVcXzG",
        "outputId": "8a52b810-09d4-4936-c082-b9df4ce94a76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 7885\n",
            "\n",
            "10 most frequent words:\n",
            "         . : 6196\n",
            "       the : 5612\n",
            "         i : 3037\n",
            "       and : 3018\n",
            "        to : 2744\n",
            "        of : 2647\n",
            "         a : 2640\n",
            "        in : 1765\n",
            "      that : 1752\n",
            "        it : 1734\n",
            "\n",
            "10 least frequent words:\n",
            "    seaman : 1\n",
            " blockaded : 1\n",
            " arguments : 1\n",
            "     locus : 1\n",
            "    standi : 1\n",
            "  survived : 1\n",
            "    solely : 1\n",
            " mauritius : 1\n",
            "manifested : 1\n",
            "   walsall : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "context_size = 5#given in example\n",
        "tokens_final = tokens\n",
        "vocab = sorted(set(tokens_final))\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "#creating the sequences\n",
        "X, y = [], []\n",
        "\n",
        "for i in range(len(tokens_final) - context_size):\n",
        "    context = tokens_final[i : i + context_size]\n",
        "    target = tokens_final[i + context_size]\n",
        "    X.append([word_to_idx[w] for w in context])\n",
        "    y.append(word_to_idx[target])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Context size: {context_size}\")\n",
        "print(f\"Total samples: {len(X)}\\n\")\n",
        "\n",
        "#verification\n",
        "for i in range(45,60):\n",
        "    context_words = [idx_to_word[idx] for idx in X[i]]\n",
        "    target_word = idx_to_word[y[i]]\n",
        "    print(f\"{' '.join(context_words)} ---> {target_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q432vmdcZAZ",
        "outputId": "dae60647-a805-4d30-e4ec-6b482860f6f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context size: 5\n",
            "Total samples: 112090\n",
            "\n",
            "that he felt any emotion ---> akin\n",
            "he felt any emotion akin ---> to\n",
            "felt any emotion akin to ---> love\n",
            "any emotion akin to love ---> for\n",
            "emotion akin to love for ---> irene\n",
            "akin to love for irene ---> adler\n",
            "to love for irene adler ---> .\n",
            "love for irene adler . ---> all\n",
            "for irene adler . all ---> emotions\n",
            "irene adler . all emotions ---> and\n",
            "adler . all emotions and ---> that\n",
            ". all emotions and that ---> one\n",
            "all emotions and that one ---> particularly\n",
            "emotions and that one particularly ---> were\n",
            "and that one particularly were ---> abhorrent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re, requests\n"
      ],
      "metadata": {
        "id": "kTTMPGtfcag7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ensuring that the GPU is used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh-H7YVYcbu5",
        "outputId": "42fc3d64-7d78-46ac-8fa2-a151ff3a9c30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "#converting to torch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long).to(device)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n"
      ],
      "metadata": {
        "id": "rSjlG9pccdH1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the model\n",
        "vocab_size=7885\n",
        "class MLPTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)           #(batch, context_size, embed_dim)\n",
        "        x = x.view(x.size(0), -1)       #flatten\n",
        "        x = self.act1(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.log_softmax(x)"
      ],
      "metadata": {
        "id": "ZTcg4IAoceco"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting training\n",
        "embed_dim = 64\n",
        "hidden_dim = 1024\n",
        "epochs = 120\n",
        "\n",
        "model = MLPTextGenerator(vocab_size, embed_dim, hidden_dim, context_size).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "crf4aAq7cfpB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_size, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.act1(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "gQwGB7A6cjhw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"vocab\": vocab,\n",
        "        \"word_to_idx\": word_to_idx,\n",
        "        \"idx_to_word\": idx_to_word,\n",
        "        \"context_size\": context_size\n",
        "    }, f)"
      ],
      "metadata": {
        "id": "ZhBYZbYQfS7X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "\n",
        "# Load vocab\n",
        "with open(\"vocab.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "vocab = data[\"vocab\"]\n",
        "vocab_size = len(vocab)\n",
        "context_size = data[\"context_size\"]\n",
        "\n",
        "# Define training variants\n",
        "variants = [\n",
        "    {\"name\": \"small\", \"embed_dim\": 32, \"hidden_dim\": 512, \"epochs\": 20},\n",
        "    {\"name\": \"medium\", \"embed_dim\": 64, \"hidden_dim\": 1024, \"epochs\": 40},\n",
        "    {\"name\": \"large\", \"embed_dim\": 128, \"hidden_dim\": 2048, \"epochs\": 60},\n",
        "]\n"
      ],
      "metadata": {
        "id": "cpAkvhV2dYLU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 512\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "for var in variants:\n",
        "    print(f\"\\nTraining {var['name']} model...\")\n",
        "\n",
        "    model = MLPTextGenerator(vocab_size, var[\"embed_dim\"], var[\"hidden_dim\"], context_size).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    epochs = var[\"epochs\"]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(xb)\n",
        "            loss = criterion(output, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        train_loss /= total\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                output = model(xb)\n",
        "                loss = criterion(output, yb)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "                preds = output.argmax(dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        val_loss /= total\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "    save_path = f\"holmes_{var['name']}.pt\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\" Saved {var['name']} model to {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mJv4wnbiigs",
        "outputId": "f24627cd-de3e-4c5e-ce71-681129a211ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training small model...\n",
            "Epoch 1/20 | Train Loss: 6.3125 | Val Loss: 5.9317 | Val Acc: 0.100\n",
            "Epoch 10/20 | Train Loss: 2.9009 | Val Loss: 6.7906 | Val Acc: 0.138\n",
            "Epoch 20/20 | Train Loss: 2.4927 | Val Loss: 7.3668 | Val Acc: 0.142\n",
            " Saved small model to holmes_small.pt\n",
            "\n",
            "Training medium model...\n",
            "Epoch 1/40 | Train Loss: 6.1307 | Val Loss: 5.7630 | Val Acc: 0.118\n",
            "Epoch 10/40 | Train Loss: 1.9245 | Val Loss: 7.2282 | Val Acc: 0.142\n",
            "Epoch 20/40 | Train Loss: 1.3984 | Val Loss: 7.8972 | Val Acc: 0.141\n",
            "Epoch 30/40 | Train Loss: 1.2417 | Val Loss: 8.1773 | Val Acc: 0.141\n",
            "Epoch 40/40 | Train Loss: 1.1789 | Val Loss: 8.3149 | Val Acc: 0.141\n",
            " Saved medium model to holmes_medium.pt\n",
            "\n",
            "Training large model...\n",
            "Epoch 1/60 | Train Loss: 6.0037 | Val Loss: 5.6392 | Val Acc: 0.136\n",
            "Epoch 10/60 | Train Loss: 0.6339 | Val Loss: 8.0381 | Val Acc: 0.134\n",
            "Epoch 20/60 | Train Loss: 0.2091 | Val Loss: 8.8432 | Val Acc: 0.137\n",
            "Epoch 30/60 | Train Loss: 0.1386 | Val Loss: 9.1154 | Val Acc: 0.136\n",
            "Epoch 40/60 | Train Loss: 0.1122 | Val Loss: 9.2701 | Val Acc: 0.138\n",
            "Epoch 50/60 | Train Loss: 0.1031 | Val Loss: 9.3429 | Val Acc: 0.138\n",
            "Epoch 60/60 | Train Loss: 0.0986 | Val Loss: 9.3831 | Val Acc: 0.137\n",
            " Saved large model to holmes_large.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "data = {\n",
        "    \"word_to_idx\": word_to_idx,\n",
        "    \"idx_to_word\": idx_to_word,\n",
        "    \"vocab\": vocab,\n",
        "    \"context_size\": context_size\n",
        "}\n",
        "with open(\"vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data, f)\n",
        "\n",
        "print(\"Saved vocab.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWad47UWjhhp",
        "outputId": "d9c24a42-bb16-4cdc-feae-715800f74311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved vocab.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"holmes_small.pt\")\n",
        "files.download(\"holmes_medium.pt\")\n",
        "files.download(\"holmes_large.pt\")\n",
        "files.download(\"vocab.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_wqehhMlluqC",
        "outputId": "d902b6fb-5827-4131-bac2-e4346b75a630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0f213b17-2e56-468d-aeae-efef9006a291\", \"holmes_small.pt\", 17519760)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ceb69c93-d19c-4ec3-a18e-5d38b4a63ca0\", \"holmes_medium.pt\", 35660443)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_793a45d4-a751-4cef-a87b-4edf58dd2134\", \"holmes_large.pt\", 73907856)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2e10a7a3-60e5-4c62-b2bf-ad7dd92968df\", \"vocab.pkl\", 241570)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1mSjVWrl5Zs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}